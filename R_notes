> x <- installed.packages(); x[ is.na(x[,"Priority"]), c("Package", "Version")]
             Package        Version    
assertthat   "assertthat"   "0.2.0"    
batch        "batch"        "1.1-4"    
BH           "BH"           "1.60.0-1" 
bindr        "bindr"        "0.1.1"    
bindrcpp     "bindrcpp"     "0.2.2"    
car          "car"          "2.1-1"    
caret        "caret"        "6.0-64"   
chron        "chron"        "2.3-47"   
cli          "cli"          "1.0.1"    
colorspace   "colorspace"   "1.2-6"    
compare      "compare"      "0.2-6"    
crayon       "crayon"       "1.3.4"    
curl         "curl"         "0.9.6"    
data.table   "data.table"   "1.9.6"    
DataCombine  "DataCombine"  "0.2.21"   
DBI          "DBI"          "0.5"      
devtools     "devtools"     "1.10.0"   
dichromat    "dichromat"    "2.0-0"    
digest       "digest"       "0.6.9"    
doParallel   "doParallel"   "1.0.10"   
dplyr        "dplyr"        "0.7.8"    
e1071        "e1071"        "1.6-7"    
fansi        "fansi"        "0.4.0"    
ggplot2      "ggplot2"      "2.1.0"    
git2r        "git2r"        "0.14.0"   
glue         "glue"         "1.3.0"    
gsubfn       "gsubfn"       "0.6-6"    
gtable       "gtable"       "0.2.0"    
httr         "httr"         "1.1.0"    
jsonlite     "jsonlite"     "0.9.19"   
labeling     "labeling"     "0.3"      
lazyeval     "lazyeval"     "0.1.10"   
lme4         "lme4"         "1.1-11"   
lubridate    "lubridate"    "1.5.0"    
magrittr     "magrittr"     "1.5"      
MatrixModels "MatrixModels" "0.4-1"    
memoise      "memoise"      "1.0.0"    
mime         "mime"         "0.4"      
minqa        "minqa"        "1.2.4"    
munsell      "munsell"      "0.4.3"    
nloptr       "nloptr"       "1.0.4"    
openssl      "openssl"      "0.9.2"    
pbkrtest     "pbkrtest"     "0.4-6"    
pillar       "pillar"       "1.3.1"    
pkgconfig    "pkgconfig"    "2.0.2"    
plogr        "plogr"        "0.2.0"    
plyr         "plyr"         "1.8.4"    
pROC         "pROC"         "1.8"      
proto        "proto"        "0.3-10"   
purrr        "purrr"        "0.2.5"    
quantreg     "quantreg"     "5.21"     
R.matlab     "R.matlab"     "3.6.0"    
R.methodsS3  "R.methodsS3"  "1.7.1"    
R.oo         "R.oo"         "1.20.0"   
R.utils      "R.utils"      "2.3.0"    
R6           "R6"           "2.3.0"    
randomForest "randomForest" "4.6-12"   
RColorBrewer "RColorBrewer" "1.1-2"    
Rcpp         "Rcpp"         "1.0.0"    
RcppEigen    "RcppEigen"    "0.3.2.8.1"
rhbase       "rhbase"       "1.2.1"    
RJSONIO      "RJSONIO"      "1.3-0"    
rlang        "rlang"        "0.3.1"    
RSQLite      "RSQLite"      "1.0.0"    
rstudioapi   "rstudioapi"   "0.5"      
scales       "scales"       "0.4.0"    
SparseM      "SparseM"      "1.7"      
sqldf        "sqldf"        "0.4-11"   
stringi      "stringi"      "1.4.3"    
stringr      "stringr"      "1.4.0"    
tibble       "tibble"       "2.0.1"    
tidyr        "tidyr"        "0.4.1"    
tidyselect   "tidyselect"   "0.2.5"    
utf8         "utf8"         "1.1.4"    
whisker      "whisker"      "0.3-2"    
withr        "withr"        "1.0.1"    
> 

R:
caret, mlr, e1071, randomForest, gbm, glm, survival
ggplot, ggplot2, lattice, matplot
lubridate, data.table, dplyr, plyr, tidyr, scales(boxcox_trans), 
jasonlite, RJSONIO, RSQLite, sqldf 
Rhbase, R.matlab  
batch, 

df = data.frame(replicate(10,sample(0:1,1000,rep=TRUE)))
df = data.frame(replicate(6,sample(10,100,rep=TRUE)))
df$newrow <- sample(100, size = nrow(df), replace = TRUE)
df = data.frame(matrix(rnorm(20), nrow=10))
df[sample(nrow(df), 3), ]


library(plyr)
ddply(df,.(ID),function(x) x[sample(nrow(x),500),])


XGBoost

Python: 
Numpy, pandas, SciPy,  StatsModels, sklearn, Matplotlib.pyplot, Seaborn, tensorflow, keras, pytorch, NLTK


import pandas as pd
import numpy as np
#using numpy's randint
df = pd.DataFrame(np.random.randint(0,100,size=(15, 4)), columns=['A','B','C','D'])
df

dfObj = pd.DataFrame(columns=['User_ID', 'UserName', 'Action'])


=============


p-value < 0.05: reject null hypothesis, > 0.05, fail to reject null hypothesis
alternative hypothesis

p-values 3 parts:
1. probability random chance would result in the observation
2. probability of observing something else that is equally rare
3. probability of observing something rarer or more extreme
One side vs two sides


p-hacking: 
1. False positive (multiple testing problem <= false discovery rete (input all p-valiues of each test => output with adjusted the p-values a little larger)
2. Add extra sample during the test (wrong) (should determine the sample size before the test)

effext size (d) = estimated difference between means / pooled estimated standard deviation or (sqrt((mean(x)^2+mean(y)^2)/2)
power = 0.8 threshold for significance alpha = 0.05 

=> sample size


Covarian = sigma (x - sample mean x) (y - sample mean y) / (n-1)

Correlation = Covariance(x, y)/( sqrt(Var(x))*sqrt(Var(y)) )

R^2 = (Var(mean) - Var(line)) / Var(mean)

Normal distribution
population mean
width of the distribution (standard deviation, sigma)
confident interval in [-2*sigma, 2*sigma]

standard deviation quatifies how the data spreads around the mean, or the variation within a set of measurements

standard error = standard deviation of the mean of the means

sample mean (mu) = sigma(x)/n
sample std = sqrt(sigma(x-mu)^2/(n-1))
variance = SS (sum of squares)/n
coefficient of variation = std / mu

population parameters

uniform distribution
expotenial distribution

central limit limit theorem - normal distribution of means (sample size >30)




先列一下ML必要知识点：
Linear Regression (L1, L2, ElasticNet, Polynomial, Time Series, Seasonal, R square, adj-R, F-statistics), 
Logistic Regression (Sigmoid, Maximum-Likelihood, Pseudo-R), 
Tree (CART, Regression Tree, Ensemble, XGB, GBM, AdaBoost, RandomForest, Entropy, Gini, Missing Value, Feature Importance, Pruning, Regularization), 
SVM (All Kernal), 
Clustering (K-mean, Hclust, DBSCAN, Distance Metrics, Silhouette Score), 
Inbalance Classification/Outlier(One Class SVM, LOF, Isolation Forest, Clustering), 
Missing Value, Feature Engineering Techniques, 
Dimension Reduction(PCA, Auto-encoder, MDS, LDA), 
Underfitting & Overfitting.

然后这是optional知识点：
NLP (Word2Vec, Sentiment), 
Neural Network(Optimizer, CNN, LSTM, Tune, Regularization, Gradient Vanishing, Activation), 
Collaborative Filtering

a/b testing，causal inference，two sample test, how to decide the time period of the test